{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g_jmLI8I_wa",
        "outputId": "38c8052f-b549-4b74-824a-c05449b9d2f6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqV3zG1QJEH5",
        "outputId": "774361b8-c4d3-40f7-94f2-0a8d8bf0384e"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==1.5.3\n",
        "!pip install tsfel\n",
        "!pip install keras_tuner\n",
        "!pip install wandb\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3e971qnJdOR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import tsfel\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import keras_tuner\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import matplotlib.cm as cm\n",
        "from sklearn import metrics\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import hashlib\n",
        "import timeit\n",
        "import math\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "\n",
        "ROOTDIR_DATASET_NORMAL = \"/content/drive/MyDrive/Kuka_v1/normal\"\n",
        "ROOTDIR_DATASET_ANOMALY = \"/content/drive/MyDrive/Kuka_v1/collisions\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_S8JiwoJhPC"
      },
      "outputs": [],
      "source": [
        "def get_df_action(filepaths_csv, filepaths_meta, action2int=None, delimiter=\";\"):\n",
        "    # Load dataframes\n",
        "    print(\"Loading data.\")\n",
        "    # Make dataframes\n",
        "    # Some classes show the output boolean parameter as True rather than true. Fix here\n",
        "    dfs_meta = list()\n",
        "    for filepath in filepaths_meta:\n",
        "        df_m = pd.read_csv(filepath, sep=delimiter)\n",
        "        df_m.str_repr = df_m.str_repr.str.replace('True', 'true')\n",
        "        df_m['filepath'] = filepath\n",
        "        dfs_meta.append(df_m)\n",
        "\n",
        "    df_meta = pd.concat(dfs_meta)\n",
        "    df_meta.index = pd.to_datetime(df_meta.init_timestamp.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "    df_meta['completed_timestamp'] = pd.to_datetime(df_meta.completed_timestamp.astype('datetime64[ms]'),\n",
        "                                                    format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "    df_meta['init_timestamp'] = pd.to_datetime(df_meta.init_timestamp.astype('datetime64[ms]'),\n",
        "                                               format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "\n",
        "    # Eventually reduce number of classes\n",
        "    # df_meta['str_repr'] = df_meta.str_repr.str.split('=', expand = True,n=1)[0]\n",
        "    # df_meta['str_repr'] = df_meta.str_repr.str.split('(', expand=True, n=1)[0]\n",
        "\n",
        "    actions = df_meta.str_repr.unique()\n",
        "    dfs = [pd.read_csv(filepath_csv, sep=\";\") for filepath_csv in filepaths_csv]\n",
        "    df = pd.concat(dfs)\n",
        "\n",
        "    # Sort columns by name !!!\n",
        "    df = df.sort_index(axis=1)\n",
        "\n",
        "    # Set timestamp as index\n",
        "    df.index = pd.to_datetime(df.time.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "    # Drop useless columns\n",
        "    columns_to_drop = [column for column in df.columns if \"Abb\" in column or \"Temperature\" in column]\n",
        "    df.drop([\"machine_nameKuka Robot_export_active_energy\",\n",
        "             \"machine_nameKuka Robot_import_reactive_energy\"] + columns_to_drop, axis=1, inplace=True)\n",
        "    signals = df.columns\n",
        "\n",
        "    df_action = list()\n",
        "    for action in actions:\n",
        "        for index, row in df_meta[df_meta.str_repr == action].iterrows():\n",
        "            start = row['init_timestamp']\n",
        "            end = row['completed_timestamp']\n",
        "            df_tmp = df.loc[start: end].copy()\n",
        "            df_tmp['action'] = action\n",
        "            # Duration as string (so is not considered a feature)\n",
        "            df_tmp['duration'] = str((row['completed_timestamp'] - row['init_timestamp']).total_seconds())\n",
        "            df_action.append(df_tmp)\n",
        "    df_action = pd.concat(df_action, ignore_index=True)\n",
        "    df_action.index = pd.to_datetime(df_action.time.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "    df_action = df_action[~df_action.index.duplicated(keep='first')]\n",
        "\n",
        "    # Drop NaN\n",
        "    df = df.dropna(axis=0)\n",
        "    df_action = df_action.dropna(axis=0)\n",
        "\n",
        "    if action2int is None:\n",
        "        action2int = dict()\n",
        "        j = 1\n",
        "        for label in df_action.action.unique():\n",
        "            action2int[label] = j\n",
        "            j += 1\n",
        "\n",
        "    df_merged = df.merge(df_action[['action']], left_index=True, right_index=True, how=\"left\")\n",
        "    # print(f\"df_merged len: {len(df_merged)}\")\n",
        "    # Where df_merged in NaN Kuka is in idle state\n",
        "    df_idle = df_merged[df_merged['action'].isna()].copy()\n",
        "    df_idle['action'] = 'idle'\n",
        "    df_idle['duration'] = df_action.duration.values.astype(float).mean().astype(str)\n",
        "    df_action = pd.concat([df_action, df_idle])\n",
        "\n",
        "    # ile label must be 0 for debug mode\n",
        "    action2int['idle'] = 0\n",
        "    print(f\"Found {len(set(df_action['action']))} different actions.\")\n",
        "    print(\"Loading data done.\\n\")\n",
        "\n",
        "    return df_action, df, df_meta, action2int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYhqSakkJvrP"
      },
      "outputs": [],
      "source": [
        "filepath_csv = [os.path.join(ROOTDIR_DATASET_NORMAL, f\"rec{r}_20220811_rbtc_0.1s.csv\") for r in [0, 2, 3, 4]]\n",
        "filepath_meta = [os.path.join(ROOTDIR_DATASET_NORMAL, f\"rec{r}_20220811_rbtc_0.1s.metadata\") for r in [0, 2, 3, 4]]\n",
        "\n",
        "filepath_csv_anomaly = [os.path.join(ROOTDIR_DATASET_ANOMALY, f\"rec{r}_collision_20220811_rbtc_0.1s.csv\") for r in [1, 5]]\n",
        "filepath_meta_anomaly = [os.path.join(ROOTDIR_DATASET_ANOMALY, f\"rec{r}_collision_20220811_rbtc_0.1s.metadata\") for r in[1, 5]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSo052BMPNB3"
      },
      "outputs": [],
      "source": [
        "def hash_string_to_float(s):\n",
        "    hash_object = hashlib.sha256(s.encode())\n",
        "    hex_dig = hash_object.hexdigest()\n",
        "    int_hash = int(hex_dig, 16)\n",
        "\n",
        "    normalized_value = int_hash / 2 ** 256\n",
        "    float_value = 2 * normalized_value - 1\n",
        "\n",
        "    return float_value\n",
        "\n",
        "def transform_datetime_strings(datetime_str_array):\n",
        "    transformed_array = []\n",
        "    for dt_str in datetime_str_array.flatten():\n",
        "        transformed_dt = datetime.strptime(dt_str, '%Y-%m-%dT%H:%M:%S.%f%z').strftime('%Y-%m-%d %H:%M:%S')\n",
        "        transformed_array.append(transformed_dt)\n",
        "    return np.array(transformed_array).reshape(datetime_str_array.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2qUnbGV1VS1"
      },
      "outputs": [],
      "source": [
        "def singature_matrix_generator(windows):\n",
        "  sensor_n = windows.shape[2] # il numero di sensori è il numero di caratteristiche meno 1\n",
        "  win_size = windows.shape[1]\n",
        "\n",
        "  # Generazione delle signature matrices\n",
        "  matrix_all = []\n",
        "  print(\"Generating signature matrices...\")\n",
        "\n",
        "  for window in windows:\n",
        "      matrix_t = np.zeros((sensor_n, sensor_n))\n",
        "      for i in range(sensor_n):\n",
        "          for j in range(i, sensor_n):\n",
        "              matrix_t[i][j] = np.inner(window[:, i], window[:, j]) / win_size  # rescale by win_size\n",
        "              matrix_t[j][i] = matrix_t[i][j]\n",
        "      matrix_all.append(matrix_t)\n",
        "  matrix_all = np.array(matrix_all)\n",
        "\n",
        "  return matrix_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mN-HmpfI75TR"
      },
      "outputs": [],
      "source": [
        "class MSCRED(tf.keras.Model):\n",
        "    def __init__(self, opt, matrixes_train, matrixes_test):\n",
        "        super(MSCRED, self).__init__()\n",
        "        # Initialize parameters\n",
        "        self.batch_size = opt['batch_size']\n",
        "        self.learning_rate = opt['learning_rate']\n",
        "        self.training_iters = opt['training_iters']\n",
        "        self.step_max = opt['step_max']\n",
        "\n",
        "        self.matrixes_train = matrixes_train\n",
        "        self.matrixes_test = matrixes_test\n",
        "        self.sensor_n = matrixes_train.shape[2]\n",
        "        self.win_size = matrixes_train.shape[1]\n",
        "\n",
        "        self.value_colnames = ['total_count', 'error_count', 'error_rate']\n",
        "        self.scale_n = len(self.value_colnames)\n",
        "\n",
        "        # Define CNN encoder layers\n",
        "        self.conv1 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='selu')\n",
        "        self.conv2 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', strides=(2, 2), activation='selu')\n",
        "        self.conv3 = tf.keras.layers.Conv2D(128, (2, 2), padding='same', strides=(2, 2), activation='selu')\n",
        "        self.conv4 = tf.keras.layers.Conv2D(256, (2, 2), padding='same', strides=(2, 2), activation='selu')\n",
        "\n",
        "        self.conv1_lstm = tf.keras.layers.ConvLSTM2D(32, (2, 2), padding='same', return_sequences=True, activation='selu')\n",
        "        self.conv2_lstm = tf.keras.layers.ConvLSTM2D(64, (2, 2), padding='same', return_sequences=True, activation='selu')\n",
        "        self.conv3_lstm = tf.keras.layers.ConvLSTM2D(128, (2, 2), padding='same', return_sequences=True, activation='selu')\n",
        "        self.conv4_lstm = tf.keras.layers.ConvLSTM2D(256, (2, 2), padding='same', return_sequences=True, activation='selu')\n",
        "\n",
        "        self.deconv4 = tf.keras.layers.Conv2DTranspose(128, (2, 2), padding='same', strides=(2, 2), activation='selu')\n",
        "        self.deconv3 = tf.keras.layers.Conv2DTranspose(64, (2, 2), padding='same', strides=(2, 2), activation='selu')\n",
        "        self.deconv2 = tf.keras.layers.Conv2DTranspose(32, (3, 3), padding='same', strides=(2, 2), activation='selu')\n",
        "        self.deconv1 = tf.keras.layers.Conv2DTranspose(self.scale_n, (3, 3), padding='same', activation='selu')\n",
        "\n",
        "    def create_sequences(self,data):\n",
        "        step_max = self.step_max\n",
        "        num_sequences = data.shape[0] - step_max + 1\n",
        "        sequences = np.zeros((num_sequences, step_max, self.sensor_n, self.sensor_n, self.scale_n))\n",
        "        for i in range(num_sequences):\n",
        "            sequences[i] = data[i:i + step_max]\n",
        "        return sequences\n",
        "    def attention_layer(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        step_max = tf.shape(inputs)[1]\n",
        "        height = tf.shape(inputs)[2]\n",
        "        width = tf.shape(inputs)[3]\n",
        "        channels = tf.shape(inputs)[4]\n",
        "\n",
        "        # Estrai l'ultimo passo temporale\n",
        "        last_output = inputs[:, -1]  # Dimensione: (batch_size, height, width, channels)\n",
        "\n",
        "        # Funzione per calcolare i punteggi di attenzione\n",
        "        def compute_attention_score(t):\n",
        "            step_output = inputs[:, t]  # Dimensione: (batch_size, height, width, channels)\n",
        "            score = tf.reduce_sum(tf.multiply(step_output, last_output), axis=[1, 2, 3])\n",
        "            return score\n",
        "\n",
        "        # Calcola i punteggi di attenzione per ogni passo temporale\n",
        "        attention_scores = tf.map_fn(compute_attention_score, tf.range(step_max), dtype=tf.float32)\n",
        "        attention_scores = tf.transpose(attention_scores, [1, 0])  # Dimensione: (batch_size, step_max)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(attention_scores, axis=1)  # Normalizza i punteggi\n",
        "\n",
        "        # Applica i pesi di attenzione\n",
        "        reshaped_inputs = tf.reshape(inputs, [batch_size, step_max, -1])  # Reshape per la moltiplicazione\n",
        "        context_vector = tf.matmul(attention_weights[:, tf.newaxis, :], reshaped_inputs)  # Dimensione: (batch_size, 1, altezza * larghezza * canali)\n",
        "        context_vector = tf.reshape(context_vector, [batch_size, height, width, channels])\n",
        "\n",
        "        return context_vector\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        step_max = input_shape[1]\n",
        "        height = input_shape[2]\n",
        "        width = input_shape[3]\n",
        "        channels = input_shape[4]\n",
        "\n",
        "        # Reshape the input to merge batch and sequence dimensions\n",
        "        reshaped_inputs = tf.reshape(inputs, (batch_size * step_max, height, width, channels))\n",
        "        # Encoder\n",
        "        conv1 = self.conv1(reshaped_inputs)\n",
        "        conv2 = self.conv2(conv1)\n",
        "        conv3 = self.conv3(conv2)\n",
        "        conv4 = self.conv4(conv3)\n",
        "\n",
        "\n",
        "        # Reshape for ConvLSTM layers\n",
        "        conv1 = tf.reshape(conv1, [-1, self.step_max, self.sensor_n, self.sensor_n, 32])\n",
        "        conv2 = tf.reshape(conv2, [-1, self.step_max, int(math.ceil(float(self.sensor_n)/2)), int(math.ceil(float(self.sensor_n)/2)), 64])\n",
        "        conv3 = tf.reshape(conv3, [-1, self.step_max, int(math.ceil(float(self.sensor_n)/4)), int(math.ceil(float(self.sensor_n)/4)), 128])\n",
        "        conv4 = tf.reshape(conv4, [-1, self.step_max, int(math.ceil(float(self.sensor_n)/8)), int(math.ceil(float(self.sensor_n)/8)), 256])\n",
        "\n",
        "        # ConvLSTM layers with attention\n",
        "        conv1_lstm_out = self.conv1_lstm(conv1)\n",
        "        conv1_lstm_out_attention= self.attention_layer(conv1_lstm_out)\n",
        "\n",
        "        conv2_lstm_out = self.conv2_lstm(conv2)\n",
        "        conv2_lstm_out_attention= self.attention_layer(conv2_lstm_out)\n",
        "\n",
        "        conv3_lstm_out = self.conv3_lstm(conv3)\n",
        "        conv3_lstm_out_attention= self.attention_layer(conv3_lstm_out)\n",
        "\n",
        "        conv4_lstm_out = self.conv4_lstm(conv4)\n",
        "        conv4_lstm_out_attention= self.attention_layer(conv4_lstm_out)\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        deconv4 = self.deconv4(conv4_lstm_out_attention)\n",
        "        deconv4 = tf.image.resize(deconv4, [int(math.ceil(float(self.sensor_n)/4)), int(math.ceil(float(self.sensor_n)/4))], method='bilinear')\n",
        "        deconv4 = tf.concat([deconv4, conv3_lstm_out_attention], axis=-1)\n",
        "\n",
        "        deconv3 = self.deconv3(deconv4)\n",
        "        deconv3 = tf.image.resize(deconv3, [int(math.ceil(float(self.sensor_n)/2)), int(math.ceil(float(self.sensor_n)/2))], method='bilinear')\n",
        "        deconv3 = tf.concat([deconv3, conv2_lstm_out_attention], axis=-1)\n",
        "\n",
        "\n",
        "        deconv2 = self.deconv2(deconv3)\n",
        "        deconv2 = tf.image.resize(deconv2, [self.sensor_n, self.sensor_n], method='bilinear')\n",
        "        deconv2 = tf.concat([deconv2, conv1_lstm_out_attention], axis=-1)\n",
        "\n",
        "        deconv1 = self.deconv1(deconv2)\n",
        "\n",
        "        return deconv1\n",
        "\n",
        "    def compile(self):\n",
        "        # Define a custom loss function if needed\n",
        "        def custom_loss(y_true, y_pred):\n",
        "            y_true_reshaped = y_true[:, -1]\n",
        "            return tf.reduce_mean(tf.square(y_true_reshaped - y_pred))\n",
        "\n",
        "        super(MSCRED, self).compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),loss = custom_loss)\n",
        "\n",
        "    def train(self):\n",
        "        train_data = np.expand_dims(self.matrixes_train, axis=-1)\n",
        "        train_data = np.tile(train_data, (1, 1, 1, 3))\n",
        "\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
        "        dataset = dataset.window(self.step_max, shift=1, drop_remainder=True)\n",
        "        dataset = dataset.flat_map(lambda window: window.batch(self.step_max))\n",
        "\n",
        "        dataset = dataset.map(lambda window: (window, window))\n",
        "        dataset = dataset.batch(self.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='loss',\n",
        "            patience=15,\n",
        "            verbose=1,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        self.fit(dataset, epochs=self.training_iters, callbacks=[early_stopping])\n",
        "\n",
        "    def test(self):\n",
        "        test_data = np.expand_dims(self.matrixes_test, axis=-1)\n",
        "        test_data = np.tile(test_data, (1, 1, 1, 3))\n",
        "\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(test_data)\n",
        "        dataset = dataset.window(self.step_max, shift=1, drop_remainder=True)\n",
        "        dataset = dataset.flat_map(lambda window: window.batch(self.step_max))\n",
        "\n",
        "        dataset = dataset.batch(self.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "        reconstructed_matrices = []\n",
        "        for batch in tqdm(dataset, desc=\"Testing\", unit=\"batch\"):\n",
        "              reconstructed_matrix = self(batch, training=False)\n",
        "              reconstructed_matrix_np = reconstructed_matrix.numpy()\n",
        "              reconstructed_matrices.append(reconstructed_matrix_np)\n",
        "\n",
        "        return np.concatenate(reconstructed_matrices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc9eCmFAAoeg"
      },
      "outputs": [],
      "source": [
        "class MSCREDnoLSTM(tf.keras.Model):\n",
        "    def __init__(self, opt, matrixes_train, matrixes_test):\n",
        "        super(MSCREDnoLSTM, self).__init__()\n",
        "        # Initialize parameters\n",
        "        self.batch_size = opt['batch_size']\n",
        "        self.learning_rate = opt['learning_rate']\n",
        "        self.training_iters = opt['training_iters']\n",
        "        self.step_max = opt['step_max']\n",
        "        self.win_size = opt['window_size']\n",
        "\n",
        "        self.matrixes_train = matrixes_train\n",
        "        self.matrixes_test = matrixes_test\n",
        "        self.sensor_n = matrixes_train.shape[2]\n",
        "        self.win_size = matrixes_train.shape[1]\n",
        "\n",
        "        self.value_colnames = ['total_count', 'error_count', 'error_rate']\n",
        "        self.scale_n = len(self.value_colnames)\n",
        "\n",
        "        # Define layers\n",
        "        self.conv1_W = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='selu')\n",
        "        self.conv2_W = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='selu', strides=(2, 2))\n",
        "        self.conv3_W = tf.keras.layers.Conv2D(128, (2, 2), padding='same', activation='selu', strides=(2, 2))\n",
        "        self.conv4_W = tf.keras.layers.Conv2D(256, (2, 2), padding='same', activation='selu', strides=(2, 2))\n",
        "\n",
        "        self.deconv4_W = tf.keras.layers.Conv2DTranspose(128, (2, 2), padding='same', activation='selu', strides=(2, 2))\n",
        "        self.deconv3_W = tf.keras.layers.Conv2DTranspose(64, (2, 2), padding='same', activation='selu', strides=(2, 2))\n",
        "        self.deconv2_W = tf.keras.layers.Conv2DTranspose(32, (3, 3), padding='same', activation='selu', strides=(2, 2))\n",
        "        self.deconv1_W = tf.keras.layers.Conv2DTranspose(self.scale_n, (3, 3), padding='same')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Encoder\n",
        "        conv1 = self.conv1_W(inputs)\n",
        "        conv2 = self.conv2_W(conv1)\n",
        "        conv3 = self.conv3_W(conv2)\n",
        "        conv4 = self.conv4_W(conv3)\n",
        "\n",
        "        # Decoder\n",
        "        deconv4 = self.deconv4_W(conv4)\n",
        "        deconv4 = tf.image.resize(deconv4, size=(conv3.shape[1], conv3.shape[2]))  # Resize to match conv3\n",
        "        deconv4 = tf.concat([deconv4, conv3], axis=-1)\n",
        "\n",
        "        deconv3 = self.deconv3_W(deconv4)\n",
        "        deconv3 = tf.image.resize(deconv3, size=(conv2.shape[1], conv2.shape[2]))  # Resize to match conv2\n",
        "        deconv3 = tf.concat([deconv3, conv2], axis=-1)\n",
        "\n",
        "        deconv2 = self.deconv2_W(deconv3)\n",
        "        deconv2 = tf.image.resize(deconv2, size=(conv1.shape[1], conv1.shape[2]))  # Resize to match conv1\n",
        "        deconv2 = tf.concat([deconv2, conv1], axis=-1)\n",
        "\n",
        "        deconv1 = self.deconv1_W(deconv2)\n",
        "        return deconv1\n",
        "\n",
        "    def compile(self):\n",
        "        super(MSCREDnoLSTM, self).compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
        "                                    loss='mean_squared_error')\n",
        "\n",
        "    def train(self):\n",
        "        train_data = np.expand_dims(self.matrixes_train, axis=-1)\n",
        "        train_data = np.tile(train_data, (1, 1, 1, 3))\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((train_data, train_data))\n",
        "        dataset = dataset.batch(self.batch_size)\n",
        "\n",
        "        self.fit(dataset, epochs=self.training_iters)\n",
        "\n",
        "    def test(self):\n",
        "        test_data = np.expand_dims(self.matrixes_test, axis=-1)\n",
        "        test_data = np.tile(test_data, (1, 1, 1, 3))\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(test_data)\n",
        "        dataset = dataset.batch(self.batch_size)\n",
        "\n",
        "        reconstructed_matrices = []\n",
        "        for batch in dataset:\n",
        "              reconstructed_matrix = self(batch, training=False)\n",
        "              reconstructed_matrix_np = reconstructed_matrix.numpy()\n",
        "              reconstructed_matrices.append(reconstructed_matrix_np)\n",
        "\n",
        "        return np.concatenate(reconstructed_matrices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJiJGABAJn2z"
      },
      "outputs": [],
      "source": [
        "def score(reconstructed_matrices, matrixes_test):\n",
        "  reconstructed_matrix_temp = np.transpose(reconstructed_matrices, [0, 3, 1, 2])\n",
        "  matrixes_test_array = np.array(matrixes_test)\n",
        "  select_matrix_error = np.square(matrixes_test_array - reconstructed_matrix_temp[:, 0, :, :])\n",
        "  scores = np.max(select_matrix_error, axis=(1, 2))\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzgaJgijz1AY"
      },
      "outputs": [],
      "source": [
        "def convert(times_windowed):\n",
        "    # Lista per memorizzare le righe\n",
        "    rows = []\n",
        "\n",
        "    for time_window in times_windowed:\n",
        "        # Estrai i timestamp di inizio e fine dalla finestra temporale\n",
        "        start = time_window[0][0]\n",
        "        end = time_window[-1][0]\n",
        "        # Aggiungi la riga alla lista\n",
        "        rows.append({'start': start, 'end': end})\n",
        "\n",
        "    # Crea il DataFrame una sola volta usando la lista di righe\n",
        "    df = pd.DataFrame(rows, columns=['start', 'end'])\n",
        "    df['start'] = pd.to_datetime(df['start'])\n",
        "    df['end'] = pd.to_datetime(df['end'])\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1O6wdKPZiaxu"
      },
      "outputs": [],
      "source": [
        "def calculate_auc(x, y):\n",
        "    x=np.array(x)\n",
        "    y=np.array(y)\n",
        "    # Ensure TPR and FPR arrays are sorted together by FPR (ascending)\n",
        "    sorted_idx = np.argsort(x)\n",
        "    x = x[sorted_idx]\n",
        "    y = y[sorted_idx]\n",
        "\n",
        "    # Calculate trapezoidal areas for each interval\n",
        "    auc = np.trapz(y, x)\n",
        "\n",
        "    return auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mO2c1P3nP9W"
      },
      "outputs": [],
      "source": [
        "def metrics_by_point_vectorized(scores, time_collision, full=False, convert=False):\n",
        "    #score is attributed to last point in window\n",
        "\n",
        "    if full:\n",
        "        thresholds = np.sort(scores)\n",
        "    else:\n",
        "        thresholds = np.linspace(scores.min(), scores.max(), num=300)\n",
        "\n",
        "    if convert:\n",
        "        time_collision = convert(time_collision) #TADGANLOADER conversion\n",
        "\n",
        "    collisions =pd.read_excel(os.path.join(ROOTDIR_DATASET_ANOMALY, \"20220811_collisions_timestamp.xlsx\"),sheet_name=None)\n",
        "    collisions = pd.concat(collisions.values(), ignore_index=True)\n",
        "    collisions_init = collisions[collisions['Inizio/fine'] == \"i\"].Timestamp - pd.to_timedelta([2] * len(collisions[collisions['Inizio/fine'] == \"i\"].Timestamp), 'h')\n",
        "    collision_end = collisions[collisions['Inizio/fine'] == \"f\"].Timestamp - pd.to_timedelta([2] * len(collisions[collisions['Inizio/fine'] == \"f\"].Timestamp), 'h')\n",
        "\n",
        "    time_collision = time_collision[:len(scores)]\n",
        "    assert len(scores) == len(time_collision), \"unmatching score/thresholds/timestamp\"\n",
        "    print(f\"--- LOADED {len(collisions_init)} COLLISIONS ---\")\n",
        "\n",
        "    # Convert timestamps to numpy arrays\n",
        "    start_times = time_collision['start'].to_numpy().astype('datetime64[ns]')\n",
        "    end_times = time_collision['end'].to_numpy().astype('datetime64[ns]')\n",
        "\n",
        "    collisions_init_np = collisions_init.to_numpy().astype('datetime64[ns]')\n",
        "    collisions_end_np = collision_end.to_numpy().astype('datetime64[ns]')\n",
        "\n",
        "    # Create a mask for each threshold\n",
        "    threshold_masks = scores[:, np.newaxis] >= thresholds\n",
        "\n",
        "    n_samples = len(scores)\n",
        "\n",
        "    # Calculate metrics for each threshold\n",
        "    results = []\n",
        "    i = 0\n",
        "    for threshold_mask in tqdm(threshold_masks.T, desc='Processing thresholds', unit='threshold'):\n",
        "\n",
        "        pos_pred  = np.sum(threshold_mask)\n",
        "        neg_pred = n_samples - pos_pred\n",
        "\n",
        "        # count anomaly timestamps included in an anomaly window -> tp\n",
        "        collision_in_window = ((start_times[threshold_mask] >= collisions_init_np[:, np.newaxis]) & \\\n",
        "                              (start_times[threshold_mask] < collisions_end_np[:, np.newaxis]))\n",
        "\n",
        "        tp = np.sum(collision_in_window) #overall sum is necessary\n",
        "        fp = pos_pred - tp\n",
        "\n",
        "        not_threshold_mask = np.where(threshold_mask, False, True)\n",
        "\n",
        "        # count non anomaly timestamps included in an anomaly window -> fn\n",
        "        false_not_collision_in_window = ((start_times[not_threshold_mask] >= collisions_init_np[:, np.newaxis]) & \\\n",
        "                                        (start_times[not_threshold_mask] < collisions_end_np[:, np.newaxis]))\n",
        "\n",
        "        fn = np.sum(false_not_collision_in_window)\n",
        "        tn = neg_pred - fn\n",
        "\n",
        "        anomaly_indices = np.where(threshold_mask)[0][np.any(collision_in_window, axis=0)]\n",
        "\n",
        "        cm_anomaly = np.array([[tn, fp], [fn, tp]])\n",
        "\n",
        "        precision = tp / (tp + fp) if tp + fp != 0 else 0\n",
        "        recall = tp / (tp + fn) if tp + fn != 0 else 0\n",
        "        fpr = fp / (fp + tn) if fp + tn != 0 else 0\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn) if tp + tn + fp + fn != 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n",
        "\n",
        "        results.append((recall, precision, fpr, accuracy, f1, cm_anomaly, anomaly_indices))\n",
        "\n",
        "    recalls, precisions, fprs, accuracies, f1s, cms, anomaly_indices_list = zip(*results)\n",
        "\n",
        "\n",
        "    return recalls, precisions, fprs, accuracies, f1s, cms, anomaly_indices_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZg27xhBY8rw"
      },
      "outputs": [],
      "source": [
        "opt = {\n",
        "    'window_size': 60,\n",
        "    'overlap_train': 0,\n",
        "    'batch_size': 4,\n",
        "    'learning_rate': 0.0005,\n",
        "    'training_iters': 150,\n",
        "    'step_max': 5\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeO2byyoYE_f",
        "outputId": "11cafde0-bcf7-432b-aeaf-3aaf06c86241"
      },
      "outputs": [],
      "source": [
        "df_action, df, df_meta, action2int = get_df_action(filepath_csv, filepath_meta)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "start_time = time.time()\n",
        "df_features = df\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "df_features.isnull().values.any()  # controllare se ci sono colonne con valori null -> ritorna true\n",
        "df_features_nonan = df_features.fillna(0)\n",
        "df_train = df_features_nonan\n",
        "\n",
        "X_train = df_train\n",
        "corr_features = tsfel.correlated_features(X_train, threshold=0.95)\n",
        "X_train.drop(corr_features, inplace=True, axis=1)\n",
        "X_train_try = X_train\n",
        "X_train_try = X_train_try.drop([\"time\"], axis=1)\n",
        "X_train_try = np.array(X_train_try)\n",
        "\n",
        "X_train_try= X_train_try[:, :-1]  # All columns except the last\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "X_train_try = scaler.fit_transform(X_train_try)\n",
        "\n",
        "window_splits = []\n",
        "#overlap = (opt['window_size'] - 15) / opt['window_size']\n",
        "window_splits.extend(\n",
        "    tsfel.utils.signal_processing.signal_window_splitter(X_train_try.copy(), opt['window_size'],opt['overlap_train']))\n",
        "window_splits = np.asarray(window_splits)\n",
        "window_splits.reshape(-1, opt['window_size'], X_train_try.shape[1])\n",
        "\n",
        "# collision dataset caricamento\n",
        "\n",
        "collisions = pd.read_excel(os.path.join(ROOTDIR_DATASET_ANOMALY, \"20220811_collisions_timestamp.xlsx\"))\n",
        "collisions_init = collisions[collisions['Inizio/fine'] == \"i\"].Timestamp - pd.to_timedelta(\n",
        "    [2] * len(collisions[collisions['Inizio/fine'] == \"i\"].Timestamp), 'h')\n",
        "collisions_end = collisions[collisions['Inizio/fine'] == \"f\"].Timestamp - pd.to_timedelta(\n",
        "    [2] * len(collisions[collisions['Inizio/fine'] == \"f\"].Timestamp), 'h')\n",
        "\n",
        "collisions_init = collisions_init.array\n",
        "collisions_init_str = [x.strftime('%Y-%m-%d %H:%M:%S') for x in collisions_init]\n",
        "collisions_init = collisions_init_str\n",
        "\n",
        "collisions_end = collisions_end.array\n",
        "\n",
        "collisions_end_str = [x.strftime('%Y-%m-%d %H:%M:%S') for x in collisions_end]\n",
        "collisions_end = collisions_end_str\n",
        "\n",
        "df_action_collision, df_collision, df_meta_collision, action2int_collision = get_df_action(filepath_csv_anomaly,filepath_meta_anomaly)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "df_features_collision = df_collision\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "df_features_collision.isnull().values.any()\n",
        "\n",
        "df_features_collision_nonan = df_features_collision.fillna(0)\n",
        "columns_to_keep = [\"time\"]\n",
        "columns_to_drop = [col for col in df_features_collision_nonan.columns if col not in columns_to_keep]\n",
        "\n",
        "X_collision = df_features_collision_nonan.drop([\"time\"], axis=1)\n",
        "df_time_only = df_features_collision_nonan.drop(columns=columns_to_drop)\n",
        "X_collision.drop(corr_features, inplace=True, axis=1)\n",
        "X_collision = np.asarray(X_collision)\n",
        "X_collision_features = X_collision[:, :-1]  # All columns except the last\n",
        "X_collision=X_collision_features\n",
        "X_collision = scaler.transform(X_collision)\n",
        "\n",
        "time_splits_test = []\n",
        "window_splits_test = []\n",
        "overlap_test = (opt['window_size'] - 1) / opt['window_size']\n",
        "window_splits_test.extend(\n",
        "    tsfel.utils.signal_processing.signal_window_splitter(X_collision.copy(), opt['window_size'], overlap_test))\n",
        "time_splits_test.extend(\n",
        "    tsfel.utils.signal_processing.signal_window_splitter(df_time_only.copy(), opt['window_size'], overlap_test))\n",
        "window_splits_test = np.asarray(window_splits_test)\n",
        "time_splits_test = np.asarray(time_splits_test)\n",
        "\n",
        "window_splits_test.reshape(-1,opt['window_size'], X_collision.shape[1])\n",
        "time_splits_test_transformed = np.array([transform_datetime_strings(window) for window in time_splits_test])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d28dee027d094e1eb9d96455fbf37e7e",
            "a73c5807d66b411d9186ff1223821aaf",
            "8a8bd057aa05453bb9b289f4633b56f7",
            "7b078c00e9af42f09a59eb62d3f1866b",
            "31e0e21c935242798a7366ad73dd4c27",
            "3cff836f743a468387c374e79f7151a8",
            "535c665909424ee8aa7caf880a912e8f",
            "98d7a2b5fe84422da6ff46a0c511b312"
          ]
        },
        "id": "nL6mS6vYCJIv",
        "outputId": "80983295-f9c9-400d-818e-54b4b1b12691"
      },
      "outputs": [],
      "source": [
        "matrixes_train = singature_matrix_generator(window_splits)\n",
        "print(matrixes_train.shape)\n",
        "matrixes_test = singature_matrix_generator(window_splits_test)\n",
        "print(matrixes_test.shape)\n",
        "\n",
        "del(window_splits)\n",
        "del(window_splits_test)\n",
        "\n",
        "model = MSCRED(opt, matrixes_train, matrixes_test)\n",
        "model.compile()\n",
        "model.train()\n",
        "\n",
        "print(\"\\n-- testing -- \" )\n",
        "reconstructed_matrices = model.test()\n",
        "print(reconstructed_matrices.shape)\n",
        "matrixes_test = matrixes_test[opt['step_max']-1:]\n",
        "print(matrixes_test.shape)\n",
        "print()\n",
        "scores = score(reconstructed_matrices,matrixes_test)\n",
        "\n",
        "del(reconstructed_matrices)\n",
        "del(matrixes_test)\n",
        "\n",
        "time_collision =  convert(time_splits_test_transformed[opt['step_max']-1:])\n",
        "thresholds = np.linspace( np.min(scores), np.max(scores), 300)\n",
        "recalls, precisions, fprs, accuracies, f1s, cms, anomaly_indices = metrics_by_point_vectorized(scores, time_collision)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "31e0e21c935242798a7366ad73dd4c27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cff836f743a468387c374e79f7151a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "535c665909424ee8aa7caf880a912e8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b078c00e9af42f09a59eb62d3f1866b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a8bd057aa05453bb9b289f4633b56f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_535c665909424ee8aa7caf880a912e8f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98d7a2b5fe84422da6ff46a0c511b312",
            "value": 1
          }
        },
        "98d7a2b5fe84422da6ff46a0c511b312": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a73c5807d66b411d9186ff1223821aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31e0e21c935242798a7366ad73dd4c27",
            "placeholder": "​",
            "style": "IPY_MODEL_3cff836f743a468387c374e79f7151a8",
            "value": "0.062 MB of 0.062 MB uploaded\r"
          }
        },
        "d28dee027d094e1eb9d96455fbf37e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a73c5807d66b411d9186ff1223821aaf",
              "IPY_MODEL_8a8bd057aa05453bb9b289f4633b56f7"
            ],
            "layout": "IPY_MODEL_7b078c00e9af42f09a59eb62d3f1866b"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
